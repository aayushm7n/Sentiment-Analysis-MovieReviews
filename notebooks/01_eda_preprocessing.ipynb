{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dda4e04",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Reviews - EDA and Preprocessing\n",
    "\n",
    "**Project**: IMDb Movie Reviews Sentiment Classification  \n",
    "**Author**: Aayushman Singh Chandel  \n",
    "**Date**: December 24, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Notebook Overview\n",
    "\n",
    "This notebook covers the initial phases of the sentiment analysis project:\n",
    "1. **Data Loading**: Loading the IMDb dataset\n",
    "2. **Exploratory Data Analysis**: Understanding data distribution and characteristics\n",
    "3. **Text Preprocessing**: Cleaning and preparing text for modeling\n",
    "4. **Feature Analysis**: Analyzing word patterns and frequencies\n",
    "5. **Data Saving**: Saving processed data for modeling\n",
    "\n",
    "**Expected Outcomes**:\n",
    "- Clean, preprocessed dataset ready for ML modeling\n",
    "- Comprehensive visualizations of data characteristics\n",
    "- Insights into positive vs. negative review patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de518ebb",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca99494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Utilities\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"Python version: 3.14\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc664d1",
   "metadata": {},
   "source": [
    "### Download NLTK Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f1e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk_resources = ['stopwords', 'punkt', 'wordnet', 'omw-1.4', 'averaged_perceptron_tagger']\n",
    "\n",
    "for resource in nltk_resources:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "        print(f\"‚úÖ Downloaded: {resource}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to download {resource}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ NLTK resources downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f043d",
   "metadata": {},
   "source": [
    "## 2. Load the IMDb Dataset\n",
    "\n",
    "We'll use the HuggingFace `datasets` library to load the standard IMDb movie reviews dataset.\n",
    "\n",
    "**Dataset Info**:\n",
    "- **Size**: 50,000 reviews (25k train, 25k test)\n",
    "- **Balance**: 50% positive, 50% negative\n",
    "- **Task**: Binary sentiment classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a54f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load IMDb dataset from HuggingFace\n",
    "print(\"Loading IMDb dataset...\")\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Display dataset structure\n",
    "print(\"\\nüìä Dataset Structure:\")\n",
    "print(dataset)\n",
    "\n",
    "# Convert to pandas DataFrames for easier manipulation\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"\\n‚úÖ Training set size: {len(train_df):,} reviews\")\n",
    "print(f\"‚úÖ Test set size: {len(test_df):,} reviews\")\n",
    "print(f\"‚úÖ Total dataset size: {len(train_df) + len(test_df):,} reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae55eacf",
   "metadata": {},
   "source": [
    "### 2.1 Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"üìã First 5 training samples:\\n\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìä Dataset Info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìà Basic Statistics:\")\n",
    "print(train_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "print(f\"Training set: {train_df.isnull().sum().sum()} missing values\")\n",
    "print(f\"Test set: {test_df.isnull().sum().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd44bbbc",
   "metadata": {},
   "source": [
    "### 2.2 Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522ac460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample positive and negative reviews\n",
    "print(\"üé¨ POSITIVE REVIEW EXAMPLE:\\n\")\n",
    "print(train_df[train_df['label'] == 1].iloc[0]['text'][:500] + \"...\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüëé NEGATIVE REVIEW EXAMPLE:\\n\")\n",
    "print(train_df[train_df['label'] == 0].iloc[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a68667",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 3.1 Sentiment Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43400706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training set\n",
    "sentiment_counts_train = train_df['label'].value_counts().sort_index()\n",
    "labels = ['Negative (0)', 'Positive (1)']\n",
    "colors = ['#ff6b6b', '#51cf66']\n",
    "\n",
    "axes[0].bar(labels, sentiment_counts_train.values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training Set - Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(sentiment_counts_train.values):\n",
    "    axes[0].text(i, v + 200, str(v), ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "# Test set\n",
    "sentiment_counts_test = test_df['label'].value_counts().sort_index()\n",
    "axes[1].bar(labels, sentiment_counts_test.values, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "axes[1].set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Test Set - Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(sentiment_counts_test.values):\n",
    "    axes[1].text(i, v + 200, str(v), ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Dataset is perfectly balanced!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af0b6f",
   "metadata": {},
   "source": [
    "### 3.2 Review Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6df68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate review lengths\n",
    "train_df['review_length'] = train_df['text'].apply(lambda x: len(str(x).split()))\n",
    "test_df['review_length'] = test_df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Overall distribution\n",
    "axes[0].hist(train_df['review_length'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Number of Words', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Review Length Distribution (Training Set)', fontsize=14, fontweight='bold')\n",
    "axes[0].axvline(train_df['review_length'].mean(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f'Mean: {train_df[\"review_length\"].mean():.1f}')\n",
    "axes[0].axvline(train_df['review_length'].median(), color='green', linestyle='--', linewidth=2,\n",
    "                label=f'Median: {train_df[\"review_length\"].median():.1f}')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# By sentiment\n",
    "for label, color, name in zip([0, 1], ['#ff6b6b', '#51cf66'], ['Negative', 'Positive']):\n",
    "    subset = train_df[train_df['label'] == label]['review_length']\n",
    "    axes[1].hist(subset, bins=50, alpha=0.6, label=name, color=color, edgecolor='black')\n",
    "\n",
    "axes[1].set_xlabel('Number of Words', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Review Length by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nüìä Review Length Statistics:\")\n",
    "print(f\"Mean: {train_df['review_length'].mean():.2f} words\")\n",
    "print(f\"Median: {train_df['review_length'].median():.2f} words\")\n",
    "print(f\"Min: {train_df['review_length'].min()} words\")\n",
    "print(f\"Max: {train_df['review_length'].max()} words\")\n",
    "print(f\"Std Dev: {train_df['review_length'].std():.2f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa3b4ce",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing\n",
    "\n",
    "We'll create a comprehensive preprocessing pipeline to clean the raw text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9214d752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "import string\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"Text preprocessing pipeline for sentiment analysis.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def clean_html(self, text):\n",
    "        \"\"\"Remove HTML tags from text.\"\"\"\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Remove URLs, special characters, and extra whitespaces.\"\"\"\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        # Remove extra whitespaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Apply full preprocessing pipeline.\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to string and lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        text = self.clean_html(text)\n",
    "        \n",
    "        # Clean text\n",
    "        text = self.clean_text(text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens \n",
    "                 if word not in self.stop_words and len(word) > 2]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "print(\"‚úÖ Text preprocessor created!\")\n",
    "print(\"\\nExample transformation:\")\n",
    "sample_text = train_df.iloc[0]['text'][:200]\n",
    "print(f\"\\nOriginal: {sample_text}...\")\n",
    "print(f\"\\nCleaned: {preprocessor.preprocess(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0750034f",
   "metadata": {},
   "source": [
    "### Apply Preprocessing to Full Dataset\n",
    "\n",
    "‚ö†Ô∏è **Note**: This may take 5-10 minutes. We'll use tqdm for progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a63cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable progress bar for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "# Process training data\n",
    "print(\"Processing training data...\")\n",
    "train_df['cleaned_text'] = train_df['text'].progress_apply(preprocessor.preprocess)\n",
    "train_df['word_count'] = train_df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "train_df['char_count'] = train_df['cleaned_text'].apply(len)\n",
    "\n",
    "# Process test data\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_df['cleaned_text'] = test_df['text'].progress_apply(preprocessor.preprocess)\n",
    "test_df['word_count'] = test_df['cleaned_text'].apply(lambda x: len(x.split()))\n",
    "test_df['char_count'] = test_df['cleaned_text'].apply(len)\n",
    "\n",
    "print(\"\\n‚úÖ Preprocessing complete!\")\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ee8c4",
   "metadata": {},
   "source": [
    "### 4.1 Word Clouds - Positive vs Negative Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds\n",
    "positive_text = ' '.join(train_df[train_df['label'] == 1]['cleaned_text'].head(5000))\n",
    "negative_text = ' '.join(train_df[train_df['label'] == 0]['cleaned_text'].head(5000))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Positive word cloud\n",
    "wc_pos = WordCloud(width=800, height=400, background_color='white',\n",
    "                   max_words=100, colormap='Greens', contour_width=2,\n",
    "                   contour_color='darkgreen').generate(positive_text)\n",
    "axes[0].imshow(wc_pos, interpolation='bilinear')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Positive Reviews - Word Cloud', fontsize=16, fontweight='bold', pad=15)\n",
    "\n",
    "# Negative word cloud\n",
    "wc_neg = WordCloud(width=800, height=400, background_color='white',\n",
    "                   max_words=100, colormap='Reds', contour_width=2,\n",
    "                   contour_color='darkred').generate(negative_text)\n",
    "axes[1].imshow(wc_neg, interpolation='bilinear')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Negative Reviews - Word Cloud', fontsize=16, fontweight='bold', pad=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/wordclouds.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Word clouds generated and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cd1174",
   "metadata": {},
   "source": [
    "### 4.2 Top Words Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def88f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get word frequencies\n",
    "from collections import Counter\n",
    "\n",
    "positive_words = Counter(' '.join(train_df[train_df['label'] == 1]['cleaned_text']).split())\n",
    "negative_words = Counter(' '.join(train_df[train_df['label'] == 0]['cleaned_text']).split())\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Positive words\n",
    "top_pos = positive_words.most_common(20)\n",
    "words_pos, counts_pos = zip(*top_pos)\n",
    "axes[0].barh(range(len(words_pos)), counts_pos, color='#51cf66', alpha=0.8, edgecolor='black')\n",
    "axes[0].set_yticks(range(len(words_pos)))\n",
    "axes[0].set_yticklabels(words_pos, fontsize=11)\n",
    "axes[0].set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Top 20 Words in Positive Reviews', fontsize=14, fontweight='bold')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Negative words\n",
    "top_neg = negative_words.most_common(20)\n",
    "words_neg, counts_neg = zip(*top_neg)\n",
    "axes[1].barh(range(len(words_neg)), counts_neg, color='#ff6b6b', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_yticks(range(len(words_neg)))\n",
    "axes[1].set_yticklabels(words_neg, fontsize=11)\n",
    "axes[1].set_xlabel('Frequency', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Top 20 Words in Negative Reviews', fontsize=14, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/top_words_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Top words comparison generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5d070",
   "metadata": {},
   "source": [
    "## 5. Save Processed Data\n",
    "\n",
    "Save the cleaned datasets for use in modeling notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a593900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "import os\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "os.makedirs('../results/figures', exist_ok=True)\n",
    "\n",
    "# Save processed data\n",
    "train_df.to_csv('../data/processed/train_processed.csv', index=False)\n",
    "test_df.to_csv('../data/processed/test_processed.csv', index=False)\n",
    "\n",
    "print(\"‚úÖ Processed data saved successfully!\")\n",
    "print(f\"\\nüìÅ Files saved:\")\n",
    "print(f\"  - ../data/processed/train_processed.csv ({len(train_df):,} rows)\")\n",
    "print(f\"  - ../data/processed/test_processed.csv ({len(test_df):,} rows)\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\nüìä Sample of processed data:\")\n",
    "print(train_df[['text', 'cleaned_text', 'label', 'word_count']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828707e4",
   "metadata": {},
   "source": [
    "## Notebook 1 Summary\n",
    "\n",
    "- Loaded 50,000 IMDb movie reviews\n",
    "- Performed comprehensive EDA\n",
    "- Analyzed sentiment distribution and review lengths\n",
    "- Preprocessed all text data (cleaning, tokenization, lemmatization)\n",
    "- Generated visualizations (word clouds, frequency plots)\n",
    "- Saved processed data for modeling\n",
    "\n",
    "**Next Steps**: Proceeding to Notebook 2 for Classical ML modeling"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
