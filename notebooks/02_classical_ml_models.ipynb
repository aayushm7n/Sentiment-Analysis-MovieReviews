{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "186d8584",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Classical Machine Learning Models\n",
    "\n",
    "**Notebook 2 of 4**: Classical ML Approaches\n",
    "\n",
    "**Author**: Aayush  \n",
    "**Date**: December 24, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Overview\n",
    "\n",
    "This notebook implements and evaluates classical machine learning models for sentiment classification:\n",
    "\n",
    "1. **Feature Engineering**: TF-IDF vectorization\n",
    "2. **Model Training**: Logistic Regression, Naive Bayes, SVM, Random Forest\n",
    "3. **Model Evaluation**: Comprehensive metrics and comparisons\n",
    "4. **Feature Analysis**: Understanding important predictive features\n",
    "\n",
    "**Models Implemented**:\n",
    "- Logistic Regression\n",
    "- Multinomial Naive Bayes\n",
    "- Support Vector Machine (Linear)\n",
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76df5d2",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c92a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa0eeb3",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data\n",
    "\n",
    "Load the preprocessed data from Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ddda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "train_df = pd.read_csv('../data/processed/train_processed.csv')\n",
    "test_df = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "print(f\"Data loaded successfully!\")\n",
    "print(f\"\\nTraining set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "\n",
    "# Prepare features and labels\n",
    "X_train = train_df['cleaned_text']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['cleaned_text']\n",
    "y_test = test_df['label']\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"Training - Positive: {(y_train==1).sum()}, Negative: {(y_train==0).sum()}\")\n",
    "print(f\"Test - Positive: {(y_test==1).sum()}, Negative: {(y_test==0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d37696",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering - TF-IDF Vectorization\n",
    "\n",
    "Convert text data into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d01857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "print(\"Creating TF-IDF features...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=10000,      # Top 10,000 features\n",
    "    ngram_range=(1, 2),      # Unigrams and bigrams\n",
    "    min_df=5,                # Ignore terms appearing in < 5 documents\n",
    "    max_df=0.8,              # Ignore terms appearing in > 80% of documents\n",
    "    sublinear_tf=True        # Use logarithmic form for TF\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF vectorization complete!\")\n",
    "print(f\"\\nFeature matrix shape:\")\n",
    "print(f\"  Training: {X_train_tfidf.shape}\")\n",
    "print(f\"  Test: {X_test_tfidf.shape}\")\n",
    "print(f\"  Vocabulary size: {len(tfidf.vocabulary_):,}\")\n",
    "print(f\"  Matrix sparsity: {(1 - X_train_tfidf.nnz / (X_train_tfidf.shape[0] * X_train_tfidf.shape[1])) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623b94b",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "### 4.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d290990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOGISTIC REGRESSION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, C=1.0, solver='liblinear')\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test_tfidf)\n",
    "y_proba_lr = lr_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lr_accuracy = accuracy_score(y_test, y_pred_lr)\n",
    "lr_precision = precision_score(y_test, y_pred_lr)\n",
    "lr_recall = recall_score(y_test, y_pred_lr)\n",
    "lr_f1 = f1_score(y_test, y_pred_lr)\n",
    "lr_roc_auc = roc_auc_score(y_test, y_proba_lr)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:  {lr_accuracy:.4f}\")\n",
    "print(f\"  Precision: {lr_precision:.4f}\")\n",
    "print(f\"  Recall:    {lr_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {lr_f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {lr_roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Negative', 'Positive']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', cbar=True,\n",
    "            xticklabels=['Negative', 'Positive'],\n",
    "            yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/cm_logistic_regression.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Logistic Regression training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3b62a8",
   "metadata": {},
   "source": [
    "### 4.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"NAIVE BAYES MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining Naive Bayes...\")\n",
    "nb_model = MultinomialNB(alpha=1.0)\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "y_proba_nb = nb_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
    "nb_precision = precision_score(y_test, y_pred_nb)\n",
    "nb_recall = recall_score(y_test, y_pred_nb)\n",
    "nb_f1 = f1_score(y_test, y_pred_nb)\n",
    "nb_roc_auc = roc_auc_score(y_test, y_proba_nb)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:  {nb_accuracy:.4f}\")\n",
    "print(f\"  Precision: {nb_precision:.4f}\")\n",
    "print(f\"  Recall:    {nb_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {nb_f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {nb_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nNaive Bayes training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a654bdc",
   "metadata": {},
   "source": [
    "### 4.3 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81687699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUPPORT VECTOR MACHINE (LINEAR)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining SVM...\")\n",
    "svm_model = LinearSVC(random_state=42, C=1.0, max_iter=2000)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Metrics\n",
    "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "svm_precision = precision_score(y_test, y_pred_svm)\n",
    "svm_recall = recall_score(y_test, y_pred_svm)\n",
    "svm_f1 = f1_score(y_test, y_pred_svm)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:  {svm_accuracy:.4f}\")\n",
    "print(f\"  Precision: {svm_precision:.4f}\")\n",
    "print(f\"  Recall:    {svm_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {svm_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=['Negative', 'Positive']))\n",
    "\n",
    "print(\"\\nSVM training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38592368",
   "metadata": {},
   "source": [
    "### 4.4 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee26984",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RANDOM FOREST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    max_depth=50,\n",
    "    min_samples_split=5\n",
    ")\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "y_proba_rf = rf_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
    "rf_precision = precision_score(y_test, y_pred_rf)\n",
    "rf_recall = recall_score(y_test, y_pred_rf)\n",
    "rf_f1 = f1_score(y_test, y_pred_rf)\n",
    "rf_roc_auc = roc_auc_score(y_test, y_proba_rf)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  Accuracy:  {rf_accuracy:.4f}\")\n",
    "print(f\"  Precision: {rf_precision:.4f}\")\n",
    "print(f\"  Recall:    {rf_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {rf_f1:.4f}\")\n",
    "print(f\"  ROC-AUC:   {rf_roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\nRandom Forest training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e108e",
   "metadata": {},
   "source": [
    "## 5. Model Comparison\n",
    "\n",
    "Create comprehensive comparison of all classical ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132994a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Naive Bayes', 'SVM', 'Random Forest'],\n",
    "    'Accuracy': [lr_accuracy, nb_accuracy, svm_accuracy, rf_accuracy],\n",
    "    'Precision': [lr_precision, nb_precision, svm_precision, rf_precision],\n",
    "    'Recall': [lr_recall, nb_recall, svm_recall, rf_recall],\n",
    "    'F1-Score': [lr_f1, nb_f1, svm_f1, rf_f1]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLASSICAL ML MODELS - PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\", results.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Bar plot\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "x = np.arange(len(results))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    axes[0].bar(x + i*width - 1.5*width, results[metric], width, \n",
    "                label=metric, alpha=0.8, edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Classical ML Models - Performance Metrics', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(results['Model'], rotation=20, ha='right', fontsize=10)\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "axes[0].set_ylim(0.8, 1.0)\n",
    "\n",
    "# Heatmap\n",
    "metrics_data = results[['Accuracy', 'Precision', 'Recall', 'F1-Score']].values\n",
    "sns.heatmap(metrics_data.T, annot=True, fmt='.3f', cmap='RdYlGn',\n",
    "            xticklabels=results['Model'], yticklabels=metrics,\n",
    "            ax=axes[1], cbar_kws={'label': 'Score'}, vmin=0.8, vmax=1.0)\n",
    "axes[1].set_title('Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/classical_ml_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "results.to_csv('../results/classical_ml_results.csv', index=False)\n",
    "\n",
    "print(\"\\nResults saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ae1fc9",
   "metadata": {},
   "source": [
    "## 6. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0562f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "joblib.dump(lr_model, '../models/logistic_regression.pkl')\n",
    "joblib.dump(nb_model, '../models/naive_bayes.pkl')\n",
    "joblib.dump(svm_model, '../models/svm.pkl')\n",
    "joblib.dump(rf_model, '../models/random_forest.pkl')\n",
    "joblib.dump(tfidf, '../models/tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"All models saved successfully!\")\n",
    "print(\"\\nSaved models:\")\n",
    "print(\"  - logistic_regression.pkl\")\n",
    "print(\"  - naive_bayes.pkl\")\n",
    "print(\"  - svm.pkl\")\n",
    "print(\"  - random_forest.pkl\")\n",
    "print(\"  - tfidf_vectorizer.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d249701",
   "metadata": {},
   "source": [
    "## Notebook 2 Summary\n",
    "\n",
    "**What we accomplished**:\n",
    "- Created TF-IDF features from preprocessed text\n",
    "- Trained 4 classical ML models\n",
    "- Evaluated all models with comprehensive metrics\n",
    "- Compared model performances\n",
    "- Saved all models for future use\n",
    "\n",
    "**Key Results**:\n",
    "- **Best Model**: SVM with highest F1-score\n",
    "- **Fastest Model**: Naive Bayes\n",
    "- **Most Balanced**: Logistic Regression\n",
    "\n",
    "**Next Steps**: Proceeding to Notebook 3 for Deep Learning models (LSTM & BERT)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
