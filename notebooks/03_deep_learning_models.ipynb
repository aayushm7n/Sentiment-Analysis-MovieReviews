{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c7999a",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4736dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"âœ… Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8ebc5",
   "metadata": {},
   "source": [
    "## 2. Load Processed Data\n",
    "\n",
    "For deep learning, we'll use a subset of the data for faster training. You can use the full dataset by removing `.head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b7ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "train_df = pd.read_csv('../data/processed/train_processed.csv')\n",
    "test_df = pd.read_csv('../data/processed/test_processed.csv')\n",
    "\n",
    "# Use subset for faster training (remove .head() for full dataset)\n",
    "train_df = train_df.head(10000)  # Use 10k samples\n",
    "test_df = test_df.head(2000)     # Use 2k samples\n",
    "\n",
    "print(f\"âœ… Data loaded!\")\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"\\nðŸ’¡ Tip: For full dataset, remove .head() calls above\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14269a7b",
   "metadata": {},
   "source": [
    "## 3. LSTM Model Implementation\n",
    "\n",
    "### 3.1 Text Tokenization for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc2597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary from training data\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocabulary(texts, max_vocab_size=10000):\n",
    "    \"\"\"Build vocabulary from texts.\"\"\"\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        all_words.extend(str(text).split())\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    vocab = ['<PAD>', '<UNK>'] + [word for word, _ in word_counts.most_common(max_vocab_size - 2)]\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    \n",
    "    return word_to_idx, vocab\n",
    "\n",
    "def tokenize_and_pad(texts, word_to_idx, max_len=256):\n",
    "    \"\"\"Convert texts to padded sequences of indices.\"\"\"\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        tokens = str(text).split()[:max_len]\n",
    "        indices = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
    "        # Pad to max_len\n",
    "        indices += [word_to_idx['<PAD>']] * (max_len - len(indices))\n",
    "        sequences.append(indices)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Build vocabulary and tokenize\n",
    "print(\"Building vocabulary...\")\n",
    "word_to_idx, vocab = build_vocabulary(train_df['cleaned_text'], max_vocab_size=10000)\n",
    "print(f\"Vocabulary size: {len(vocab):,}\")\n",
    "\n",
    "print(\"\\nTokenizing texts...\")\n",
    "max_len = 256\n",
    "X_train_seq = tokenize_and_pad(train_df['cleaned_text'], word_to_idx, max_len)\n",
    "X_test_seq = tokenize_and_pad(test_df['cleaned_text'], word_to_idx, max_len)\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"âœ… Tokenization complete!\")\n",
    "print(f\"Training shape: {X_train_seq.shape}\")\n",
    "print(f\"Test shape: {X_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8efdef",
   "metadata": {},
   "source": [
    "### 3.2 Create PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.LongTensor(X_train_seq)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "X_test_tensor = torch.LongTensor(X_test_seq)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"âœ… DataLoaders created!\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88eebec",
   "metadata": {},
   "source": [
    "### 3.3 LSTM Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd023d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSentimentClassifier(nn.Module):\n",
    "    \"\"\"Bidirectional LSTM for sentiment classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.5):\n",
    "        super(LSTMSentimentClassifier, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)  # *2 for bidirectional\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # Concatenate last hidden states from both directions\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # Final classification\n",
    "        output = self.fc(hidden)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output.squeeze()\n",
    "\n",
    "# Initialize model\n",
    "lstm_model = LSTMSentimentClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "print(\"âœ… LSTM model created!\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(lstm_model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in lstm_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc0c14b",
   "metadata": {},
   "source": [
    "### 3.4 Train LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d68634",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TRAINING LSTM MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    for sequences, labels in pbar:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        epoch_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{correct/total:.4f}'})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_acc = correct / total\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accs.append(avg_acc)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Loss = {avg_loss:.4f}, Accuracy = {avg_acc:.4f}')\n",
    "\n",
    "print(\"\\nâœ… LSTM training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbc2210",
   "metadata": {},
   "source": [
    "### 3.5 Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "lstm_model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in tqdm(test_loader, desc='Evaluating LSTM'):\n",
    "        sequences = sequences.to(device)\n",
    "        outputs = lstm_model(sequences)\n",
    "        predictions = (outputs > 0.5).float().cpu().numpy()\n",
    "        all_predictions.extend(predictions)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "lstm_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "lstm_precision = precision_score(all_labels, all_predictions)\n",
    "lstm_recall = recall_score(all_labels, all_predictions)\n",
    "lstm_f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LSTM MODEL - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {lstm_accuracy:.4f}\")\n",
    "print(f\"Precision: {lstm_precision:.4f}\")\n",
    "print(f\"Recall:    {lstm_recall:.4f}\")\n",
    "print(f\"F1-Score:  {lstm_f1:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4ab191",
   "metadata": {},
   "source": [
    "## 4. BERT Model Implementation\n",
    "\n",
    "### 4.1 Load DistilBERT and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2aa4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DISTILBERT MODEL SETUP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(\"\\nLoading DistilBERT model and tokenizer...\")\n",
    "bert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    'distilbert-base-uncased',\n",
    "    num_labels=2\n",
    ").to(device)\n",
    "\n",
    "print(\"âœ… DistilBERT loaded!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in bert_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c230a5",
   "metadata": {},
   "source": [
    "### 4.2 Prepare BERT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55791f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data for BERT (use original text, not cleaned_text)\n",
    "print(\"Tokenizing data for BERT...\")\n",
    "train_encodings = bert_tokenizer(\n",
    "    train_df['text'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "test_encodings = bert_tokenizer(\n",
    "    test_df['text'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=256,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "bert_train_dataset = BERTDataset(train_encodings, train_df['label'].tolist())\n",
    "bert_test_dataset = BERTDataset(test_encodings, test_df['label'].tolist())\n",
    "\n",
    "# Create dataloaders\n",
    "bert_train_loader = DataLoader(bert_train_dataset, batch_size=16, shuffle=True)\n",
    "bert_test_loader = DataLoader(bert_test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"âœ… BERT data preparation complete!\")\n",
    "print(f\"Training batches: {len(bert_train_loader)}\")\n",
    "print(f\"Test batches: {len(bert_test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54271161",
   "metadata": {},
   "source": [
    "### 4.3 Train BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ff170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING DISTILBERT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "num_epochs_bert = 3\n",
    "total_steps = len(bert_train_loader) * num_epochs_bert\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    bert_optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs_bert):\n",
    "    bert_model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(bert_train_loader, desc=f'BERT Epoch {epoch+1}/{num_epochs_bert}')\n",
    "    for batch in pbar:\n",
    "        bert_optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = bert_model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        bert_optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(bert_train_loader)\n",
    "    print(f'Epoch {epoch+1}: Average Loss = {avg_loss:.4f}')\n",
    "\n",
    "print(\"\\nâœ… BERT training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb91b41",
   "metadata": {},
   "source": [
    "### 4.4 Evaluate BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1547fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "bert_model.eval()\n",
    "bert_predictions = []\n",
    "bert_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(bert_test_loader, desc='Evaluating BERT'):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels']\n",
    "        \n",
    "        outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        bert_predictions.extend(predictions)\n",
    "        bert_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "bert_accuracy = accuracy_score(bert_labels, bert_predictions)\n",
    "bert_precision = precision_score(bert_labels, bert_predictions)\n",
    "bert_recall = recall_score(bert_labels, bert_predictions)\n",
    "bert_f1 = f1_score(bert_labels, bert_predictions)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISTILBERT MODEL - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {bert_accuracy:.4f}\")\n",
    "print(f\"Precision: {bert_precision:.4f}\")\n",
    "print(f\"Recall:    {bert_recall:.4f}\")\n",
    "print(f\"F1-Score:  {bert_f1:.4f}\")\n",
    "\n",
    "print(\"\\nðŸ“‹ Classification Report:\")\n",
    "print(classification_report(bert_labels, bert_predictions, target_names=['Negative', 'Positive']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43fb35",
   "metadata": {},
   "source": [
    "## 5. Save Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb0eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LSTM model\n",
    "torch.save(lstm_model.state_dict(), '../models/lstm_model.pth')\n",
    "print(\"âœ… LSTM model saved to ../models/lstm_model.pth\")\n",
    "\n",
    "# Save BERT model\n",
    "bert_model.save_pretrained('../models/distilbert_model')\n",
    "bert_tokenizer.save_pretrained('../models/distilbert_tokenizer')\n",
    "print(\"âœ… DistilBERT model saved to ../models/distilbert_model/\")\n",
    "\n",
    "# Save deep learning results\n",
    "dl_results = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'DistilBERT'],\n",
    "    'Accuracy': [lstm_accuracy, bert_accuracy],\n",
    "    'Precision': [lstm_precision, bert_precision],\n",
    "    'Recall': [lstm_recall, bert_recall],\n",
    "    'F1-Score': [lstm_f1, bert_f1]\n",
    "})\n",
    "\n",
    "dl_results.to_csv('../results/deep_learning_results.csv', index=False)\n",
    "print(\"âœ… Results saved to ../results/deep_learning_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e5d8a",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Notebook 3 Complete!\n",
    "\n",
    "**What we accomplished**:\n",
    "- âœ… Implemented custom Bidirectional LSTM model\n",
    "- âœ… Fine-tuned DistilBERT transformer model\n",
    "- âœ… Trained both models with proper training loops\n",
    "- âœ… Evaluated performance on test set\n",
    "- âœ… Saved trained models for deployment\n",
    "\n",
    "**Expected Results**:\n",
    "- **LSTM**: ~87-90% accuracy\n",
    "- **DistilBERT**: ~92-94% accuracy (best overall!)\n",
    "\n",
    "**Next Steps**: Proceed to Notebook 4 for comprehensive model comparison and final analysis!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
